# [T16.00] Glue Spark: integrating data

## Documentation:

- [DynamicFrameReader class]()
- [pyspark.sql.DataFrame.join]()
- [pyspark.sql.functions.md5]()
- [Using the Parquet format in AWS Glue]()
- [AWS::Glue::Job]()

## Description:

Create AWS Glue Spark Job that reads fact_go_daily_sales data from AWS S3 Stage bucket and dimensions from AWS S3 Analytics bucket. Integrate data and load to AWS S3 Analytics bucket in parquet format.

Files for pushing to CodeComet:

- Script name: `fact_go_daily_sales_analytics_t11.py`
- CloudFormation template: `{user_id}_fact_go_daily_sales_analytics_t11.yaml`

## Configuration:

1. Path: `configurations/fact_go_daily_sales.yaml`
2. Use `stage-to-analytics` section
3. Read data from `[data-source][s3]` for fact, mapping and dimensions accordingly
4. Write data to `[data-target][s3][keys]`
5. Replace `{user_id}` with your aws account name for target path

## Glue Parameters:

1. To represent Glue Flow developing locally you need to set parameters as it would on the cluster.
2. You can do this with sys.argv.append command before using getResolvedOptions function from awsglue folder in your repository.
3. Parameters that you will need to set:

- **UserId** = `{user_id}`
- **ConfigBucketKey** = `configurations/{configuration_file_name}.yaml`
- **ConfigBucketName** = `aws-data-engineering-course-{user-id}-assets`

## Local Development:

1. Ingest config file from S3 using commons package in your repository.
2. Set spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic") option.
3. Read fact data from stage (source) and analytics (target) zones using DynamicFrame reader class and find out new data based on ingest_dt column values. You should load only new data to the target.
4. Read dimensions and mapping using dynamic frame reader class.
5. Join fact with dimensions by columns below to get SK and version columns if exist:
   1. _dimension: go_methods(columns_to_join: 'order_method_type')_
   2. _dimension: time_period(columns_to_join: 'date')_
   
   To integrate currency dimension with fact use 'country' column from mapping file.
   3. _dimension: currency(columns_to_join_with_mapping: 'currency_alpha_2'; columns_to_join_with_fact: 'country', 'date')_
6. Generate `sk_daily_sales` using md5 transformation based on all columns
7. Drop duplicates and nulls if needed.
8. Add column 'audit_ts' that is equal to current timestamp.
9. Load data to the target with the next schema partitioning by ingest_dt.
```
root
 |-- sk_daily_sales: string (nullable = false)
 |-- sk_order_method: string (nullable = true)
 |-- sk_time_period: string (nullable = true)
 |-- sk_currency: string (nullable = true)
 |-- currency_version: integer (nullable = true)
 |-- quantity: integer (nullable = true)
 |-- unit_price: decimal(10,2) (nullable = true)
 |-- unit_sale_price: decimal(10,2) (nullable = true)
 |-- audit_ts: timestamp (nullable = false)
 |-- ingest_dt: date (nullable = true)
```

## Amazon Development:

1. Create a Glue Spark Job on cluster with the name: `{user_id}_fact_go_daily_sales_analytics_t11`.
2. **Mandatory Steps:**
   1. **Choose Python 3.9 for Python version**
   2. **Choose 1/16 DPU for data processing units**
   3. **Choose 0 retries**
   4. **Set timeout for 5**
   5. **Set your bucket path for Script path**
   6. **Set parameters**
   7. **Set Tags Owner=aws-edu-iba-gomel and StudentName={user_id}**
3. Add necessary options to create a Glue Spark Job, test a Glue Spark Job and check logs in AWS CloudWatch.
4. Create AWS CloudFormation template for Glue Spark Job with the name: `{user_id}_fact_go_daily_sales_analytics_t11.yaml`.
5. Test CloudFormation Template:
   1. Deploy (check that your Glue Python Shell has been created using CloudFormation)
   2. Delete your CloudFormation Stack
6. Open Pull Request as described in Common Info (Task Workflow) section.