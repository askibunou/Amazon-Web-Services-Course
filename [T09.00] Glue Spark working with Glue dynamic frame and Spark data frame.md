# [T09.00] Glue Spark: working with Glue dynamic frame and Spark data frame

## Documentation:

- [pyspark.sql.functions.to_json](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.to_json.html)
- [Spark from_json() â€“ Convert JSON Column](https://sparkbyexamples.com/spark/spark-from_json-convert-json-column-to-struct-map-or-multiple-columns/amp/)
- [Using the CSV format in AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-csv-home.html)
- [Using the JSON format in AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-json-home.html)
- [DynamicFrameReader class](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame-reader.html)
- [AWS::Glue::Job](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-glue-job.html)

## Description:

Create AWS Glue Spark Job that loads dim_currency data from raw S3 bucket to stage S3 bucket in parquet format.

Files for pushing to CodeComet:

- Script name: `dim_currency_stage_t4.py`
- CloudFormation template: `{user_id}_dim_currency_stage_t4.yaml`

## Configuration:

1. Path: configurations/dim_currency.yaml
2. Use raw-to-stage section
3. Read data from [data-source][s3][keys][currency] for currencies
4. Read data from [data-source][s3][keys][rates] for rates
5. Replace {user_id} with your aws account name for target path

## Glue Parameters:

1. To represent Glue Flow developing locally you need to set parameters as it would on the cluster.
2. You can do this with sys.argv.append command before using getResolvedOptions function from awsglue folder in your repository.
3. Parameters that you will need to set:

- **UserId** = `{user_id}`
- **ConfigBucketKey** = `configurations/{configuration_file_name}.yaml`
- **ConfigBucketName** = `aws-data-engineering-course-{user-id}-assets`

## Local Development:

1. Ingest config file from S3 using commons package in your repository.
2. Get S3 paths with max ingest_date for currency and rates data, using S3 client from commons package.
3. Read currency and rates data from S3 paths found in step 2 using glue dynamic frame reader class.
4. Transform json data for rates to get fields as in example below.
```
+--------------------+-----------+---------+------+-----------+--------------------+
|    currency_name   | rate_date |   rate  | base | ingest_dt |       audit_ts     |
+--------------------+-----------+---------+------+-----------+--------------------+
|          AUD       | 2015-01-01| 1.227915|  USD |2022-10-11 |2022-11-23 08:06:...|
|          EUR       | 2015-01-01| 0.830151|  USD |2022-10-11 |2022-11-23 08:06:...|
|          BRL       | 2015-01-01| 2.65757 |  USD |2022-10-11 |2022-11-23 08:06:...|
```
5. Join currency df with rates df by currency_name field.
6. Drop duplicates and nulls if needed.
7. Add column 'ingest_dt' that is equal to max ingest_dt from the read path.
8. Add column 'audit_ts' that is equal to current timestamp.
9. Load data to the target with the next schema.
```
root
 |-- currency_name: string (nullable = true)
 |-- currency_fullname: string (nullable = true)
 |-- currency_alpha_2: string (nullable = true)
 |-- rate_date: date (nullable = true)
 |-- rate: double (nullable = true)
 |-- base: string (nullable = true)
 |-- ingest_dt: date (nullable = true)
 |-- audit_ts: timestamp (nullable = false)
```

**Output example:**
```
+-------------------+-----------------------+-----------------------+------------+--------------+------+------------+--------------------+
|   currency_name   |   currency_fullname   |     currency_alpha_2  | rate_date  |     rate     | base | ingest_dt  |      audit_ts      |
+-------------------+-----------------------+-----------------------+------------+--------------+------+------------+--------------------+
|          DKK      |    Denmark Krone      |           DK          | 2018-12-31 |   6.511396   | USD  | 2022-10-11 |2022-11-23 08:06:...|
```

## Amazon Development:

1. Create a Glue Spark Job on cluster with the name: `{user_id}_dim_currency_stage_t4`.
2. **Mandatory Steps:**
   1. **Choose Python 3.9 for Python version**
   2. **Choose 1/16 DPU for data processing units**
   3. **Choose 0 retries**
   4. **Set timeout for 5**
   5. **Set your bucket path for Script path**
   6. **Set parameters**
   7. **Set Tags Owner=aws-edu-iba-gomel and StudentName={user_id}**
3. Add necessary options to create a Glue Spark Job, test a Glue Spark Job and check logs in AWS CloudWatch.
4. Create AWS CloudFormation template for Glue Spark Job with the name: `{user_id}_dim_currency_stage_t4.yaml`.
5. Test CloudFormation Template:
   1. Deploy (check that your Glue Python Shell has been created using CloudFormation)
   2. Delete your CloudFormation Stack
6. Open Pull Request as described in Common Info (Task Workflow) section.