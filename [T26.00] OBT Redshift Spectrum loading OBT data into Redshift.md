# [T26.00] OBT Redshift Spectrum: loading OBT data into Redshift

## Documentation:

- [Examples of using the Amazon Redshift Python connector](https://docs.aws.amazon.com/redshift/latest/mgmt/python-connect-examples.html)

## Description:

Create a Python Job that loads data from Redshift Spectrum table to Redshift table. We need Spark Job in that case to launch script from EMR further in the other case we would use just Python.

Files for pushing to CodeComet:

- DDL name: `OBT.sql`
- Script name: `obt_datawarehouse.py`

## Configuration:

1. Path: `configurations/obt.yaml`
2. Use `datawarehouse` section
3. Use `[redshift-connect]` to connect to redshift

## Environment Variables:

- **UserId** = `{user_id}`
- **ConfigBucketKey** = `configurations/{configuration_file_name}.yaml`
- **ConfigBucketName** = `aws-data-engineering-course-{user-id}-assets`

## Local Development:

1. Create table in schema `{user_id}` named 'obt' as you created tables in task ...'create redshift tables'
2. Ingest config file from S3 using commons.
3. Connect to redshift using redshift_connector.
4. Create such workflow to load data:
   1. Create temporary table as target table.
   2. Load data to the temp table from Spectrum table.
   3. Rename target table to obt_to_rm
   4. Rename temp table to target table name
   5. Delete table from step 3.