# [T20.00] Glue Redshift: loading data from s3 to redshift

## Documentation:

- [Examples of using the Amazon Redshift Python connector](https://docs.aws.amazon.com/redshift/latest/mgmt/python-connect-examples.html)
- [COPY](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html)
- [AWS::Glue::Job](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-glue-job.html)

## Description:

Create generic AWS Glue Python Shell Job that loads dimensions (`dim_go_methods`, `dim_currency`, `dim_time_period`) data from AWS S3 Analytics to Redshift Tables.

Files for pushing to CodeComet:

- Script name: `dim_redshift_load_t14.py`
- CloudFormation template: `{user_id}_dim_go_methods_datawarehouse_t14.yaml`, `{user_id}_dim_currency_datawarehouse_t14.yaml`, `{user_id}_dim_time_period_datawarehouse_t14.yaml`

## Configuration:

1. Path: configurations/{any_dimensions}.yaml
2. Use `analytics-to-datawarehouse` section
3. Read data from `[data-source][s3][keys]`
4. Write data to `[data-target][redshift]`
5. Replace `{user_id}` with your aws account name for target path

## Glue Parameters:

1. To represent Glue Flow developing locally you need to set parameters as it would on the cluster.
2. You can do this with sys.argv.append command before using getResolvedOptions function from awsglue folder in your repository.
3. Parameters that you will need to set:

- **UserId** = `{user_id}`
- **ConfigBucketKey** = `configurations/{configuration_file_name}.yaml`
- **ConfigBucketName** = `aws-data-engineering-course-{user-id}-assets`

## Local Development:

1. Ingest config file from S3 using commons package in your repository.
2. Get credentials from AWS SecretsManager using SecretId that is stored in config.
3. Connect to redshift using redshift_connector.
4. Create such workflow to load data:
   1. Create temporary table as target table.
   2. Load data to the temp table using command `COPY`.
   3. Rename target table to `{table}_to_rm`.
   4. Rename temp table to target table name.
   5. Delete table from step 3.

## Amazon Development:

1. Create a Glue Python Shell on cluster with the name: `{user_id}_dim_go_methods_datawarehouse_t14`, `{user_id}_dim_currency_datawarehouse_t14`, `{user_id}_dim_time_period_datawarehouse_t14`
2. **Mandatory Steps:**
   1. **Choose Python 3.9 for Python version**
   2. **Choose 1/16 DPU for data processing units**
   3. **Choose 0 retries**
   4. **Set timeout for 5**
   5. **Set your bucket path for Script path**
   6. **Set parameters**
   7. **Set Tags Owner=aws-edu-iba-gomel and StudentName={user_id}**
3. Add necessary options to create a Glue Spark Job, test a Glue Spark Job and check logs in AWS CloudWatch.
4. Create AWS CloudFormation template for Glue Spark Job with the name: `{user_id}_dim_go_methods_datawarehouse_t14.yaml`, `{user_id}_dim_currency_datawarehouse_t14.yaml`, `{user_id}_dim_time_period_datawarehouse_t14.yaml`.
5. Test CloudFormation Template:
   1. Deploy (check that your Glue Python Shell has been created using CloudFormation)
   2. Delete your CloudFormation Stack
6. Open Pull Request as described in Common Info (Task Workflow) section.